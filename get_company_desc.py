"""
Yahoo Finance å…¬å¸æè¿°ç²å–å™¨
"""
import requests
from bs4 import BeautifulSoup
import time
import random
from typing import Optional
import re


class CompanyDescScraper:
    """
    å¾Yahoo Financeç²å–å…¬å¸æè¿°ä¿¡æ¯
    """
    
    def __init__(self):
        self.session = requests.Session()
        # è¨­ç½®User-Agentä¾†æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_company_description(self, symbol: str, region: str = "ca") -> Optional[str]:
        """
        ç²å–å…¬å¸æè¿°
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼
            region: åœ°å€ä»£ç¢¼ (ca, us, etc.)
            
        Returns:
            str: å…¬å¸æè¿°ï¼Œå¦‚æœç²å–å¤±æ•—å‰‡è¿”å›None
        """
        try:
            # æ§‹å»ºURL
            url = f"https://{region}.finance.yahoo.com/quote/{symbol.upper()}/profile/"
            
            print(f"ğŸ” æ­£åœ¨ç²å– {symbol} çš„å…¬å¸æè¿°...")
            print(f"URL: {url}")
            
            # æ·»åŠ éš¨æ©Ÿå»¶é²é¿å…è¢«å°IP
            time.sleep(random.uniform(1, 3))
            
            # ç™¼é€è«‹æ±‚
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æ–¹æ³•1: å°‹æ‰¾ data-testid="description" çš„å…ƒç´ 
            desc_element = soup.find(attrs={"data-testid": "description"})
            
            if desc_element:
                # æŸ¥æ‰¾å…¶ä¸­çš„ <p> æ¨™ç±¤
                p_tag = desc_element.find('p')
                if p_tag:
                    description = p_tag.get_text(strip=True)
                    if description:
                        print(f"âœ… æˆåŠŸç²å–æè¿° (æ–¹æ³•1)")
                        return self._clean_description(description)
            
            # æ–¹æ³•2: å‚™ç”¨æ–¹æ¡ˆ - å°‹æ‰¾åŒ…å«å…¬å¸æè¿°çš„å…¶ä»–å¯èƒ½å…ƒç´ 
            alternative_selectors = [
                'span[data-testid="description"]',
                'div[data-testid="description"] p',
                '.quote-sub-section p',
                '.asset-profile-container p',
                '.Mt\\(15px\\) p'
            ]
            
            for selector in alternative_selectors:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        text = element.get_text(strip=True)
                        if text and len(text) > 50:  # ç¢ºä¿æ˜¯å¯¦è³ªæ€§çš„æè¿°
                            print(f"âœ… æˆåŠŸç²å–æè¿° (å‚™ç”¨æ–¹æ¡ˆ: {selector})")
                            return self._clean_description(text)
                except Exception:
                    continue
            
            # æ–¹æ³•3: æœç´¢åŒ…å«é—œéµè©çš„æ®µè½
            all_paragraphs = soup.find_all('p')
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                # æª¢æŸ¥æ˜¯å¦åƒå…¬å¸æè¿°ï¼ˆåŒ…å«é—œéµè©ä¸”é•·åº¦åˆé©ï¼‰
                if (len(text) > 100 and 
                    any(keyword in text.lower() for keyword in 
                        ['company', 'corporation', 'business', 'operates', 'provides', 'engages', 'develops'])):
                    print(f"âœ… æˆåŠŸç²å–æè¿° (é—œéµè©åŒ¹é…)")
                    return self._clean_description(text)
            
            print(f"âŒ æœªæ‰¾åˆ° {symbol} çš„å…¬å¸æè¿°")
            return None
            
        except requests.RequestException as e:
            print(f"âŒ ç¶²çµ¡è«‹æ±‚å¤±æ•—: {e}")
            return None
        except Exception as e:
            print(f"âŒ è§£æå¤±æ•—: {e}")
            return None
    
    def _clean_description(self, description: str) -> str:
        """
        æ¸…ç†æè¿°æ–‡æœ¬
        
        Args:
            description: åŸå§‹æè¿°
            
        Returns:
            str: æ¸…ç†å¾Œçš„æè¿°
        """
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        description = re.sub(r'\s+', ' ', description)
        
        # ç§»é™¤å¯èƒ½çš„HTMLæ®˜ç•™
        description = re.sub(r'<[^>]+>', '', description)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        description = description.strip()
        
        return description
    
    def get_multiple_descriptions(self, symbols: list, region: str = "ca") -> dict:
        """
        æ‰¹é‡ç²å–å¤šå€‹å…¬å¸çš„æè¿°
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
            region: åœ°å€ä»£ç¢¼
            
        Returns:
            dict: {symbol: description} çš„å­—å…¸
        """
        results = {}
        
        for i, symbol in enumerate(symbols):
            print(f"\nğŸ“‹ è™•ç† {i+1}/{len(symbols)}: {symbol}")
            
            description = self.get_company_description(symbol, region)
            results[symbol.upper()] = description
            
            # æ·»åŠ å»¶é²é¿å…è¢«é™åˆ¶
            if i < len(symbols) - 1:
                delay = random.uniform(2, 5)
                print(f"â³ ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
        
        return results
    
    def close(self):
        """é—œé–‰session"""
        self.session.close()


# æ¸¬è©¦ç”¨ä¾‹
if __name__ == "__main__":
    scraper = CompanyDescScraper()
    
    try:
        print("ğŸš€ Yahoo Finance å…¬å¸æè¿°ç²å–å™¨æ¸¬è©¦")
        print("=" * 50)
        
        # æ¸¬è©¦å–®å€‹è‚¡ç¥¨
        test_symbol = "XPON"
        print(f"\nğŸ§ª æ¸¬è©¦ 1: ç²å– {test_symbol} çš„æè¿°")
        description = scraper.get_company_description(test_symbol)
        
        if description:
            print(f"\nğŸ“ {test_symbol} å…¬å¸æè¿°:")
            print("-" * 30)
            print(description)
            print(f"\nğŸ“Š æè¿°é•·åº¦: {len(description)} å­—ç¬¦")
        else:
            print(f"âŒ ç„¡æ³•ç²å– {test_symbol} çš„æè¿°")
        
        # æ¸¬è©¦å¤šå€‹è‚¡ç¥¨
        test_symbols = ["AAPL", "TSLA", "MSFT"]
        print(f"\nğŸ§ª æ¸¬è©¦ 2: æ‰¹é‡ç²å–æè¿°")
        print(f"æ¸¬è©¦è‚¡ç¥¨: {', '.join(test_symbols)}")
        
        descriptions = scraper.get_multiple_descriptions(test_symbols)
        
        print(f"\nğŸ“‹ æ‰¹é‡ç²å–çµæœ:")
        print("-" * 50)
        for symbol, desc in descriptions.items():
            if desc:
                print(f"\nâœ… {symbol}:")
                print(f"   {desc[:100]}..." if len(desc) > 100 else f"   {desc}")
            else:
                print(f"\nâŒ {symbol}: ç²å–å¤±æ•—")
        
        # çµ±è¨ˆçµæœ
        success_count = sum(1 for desc in descriptions.values() if desc)
        print(f"\nğŸ“Š æˆåŠŸç‡: {success_count}/{len(test_symbols)} ({success_count/len(test_symbols)*100:.1f}%)")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        scraper.close()
        print("\nğŸ æ¸¬è©¦å®Œæˆ")